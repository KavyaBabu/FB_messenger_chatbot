# -*- coding: utf-8 -*-
"""fake_news_detection_lstm_cnn_model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12R6Efw89SORaOcbA7uEbEPZ0rgMzBnVR

## **Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re, string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud
from keras.preprocessing.text import Tokenizer
import gensim
from gensim.models import Word2Vec
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Dense,Embedding,LSTM, Conv1D, Dropout, MaxPooling1D, AvgPool1D
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
import pickle

true_data = pd.read_csv('./True.csv')
fake_data = pd.read_csv('./Fake.csv')

true_data['label'] = 1;

fake_data['label'] = 0;

publisher = []
tmp_text = []

for index, row in enumerate(true_data.text.values):
  if ' - ' in row:
    record = row.split(' - ',maxsplit=1)
    publisher.append(record[0].strip())
    tmp_text.append(record[1].strip())
  else:
    publisher.append('unknown')
    tmp_text.append(row)

true_data['publisher'] = publisher
true_data['text'] = tmp_text

true_data['text'] = true_data['title'] + " " + true_data['text']
fake_data['text'] = fake_data['title'] + " " + fake_data['text']

true_data['text'] = true_data['text'].apply(lambda x: str(x).lower())
fake_data['text'] = fake_data['text'].apply(lambda x: str(x).lower())

true_data = true_data[['text','label']]

fake_data = fake_data[['text','label']]

news_data = true_data._append(fake_data,ignore_index=True)

news_data['processed_text'] = news_data['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x))

label = news_data['label'].values

data = [d.split() for d in news_data['processed_text'].tolist()]

w2vModel = gensim.models.Word2Vec(sentences=data,
                                  vector_size=100,
                                  window=10,
                                  min_count=1)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(data)
data = tokenizer.texts_to_sequences(data)
data = pad_sequences(data,maxlen = 1000)

with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

vocab_size = len(tokenizer.word_index) + 1
vocab = tokenizer.word_index

def get_weight_matrix(model):
  weight_matrix = np.zeros((vocab_size, 100))

  for word, i in vocab.items():
    weight_matrix[i] = model.wv[word]

  return weight_matrix

embedding_vector = get_weight_matrix(w2vModel)

"""### **Architect Model with CNN & LSTM**"""

# model = Sequential()
# model.add(Embedding(vocab_size,output_dim=100, weights = [embedding_vector], input_length= 1000, trainable= False))
# model.add(Conv1D(32,4,activation='relu'))
# model.add(AvgPool1D())
# model.add(Conv1D(64,4,activation='relu'))
# model.add(AvgPool1D())
# model.add(LSTM(units=128))
# model.add(Dense(1, activation='sigmoid'))
# model.summary()
# model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])

model = Sequential()
model.add(Embedding(vocab_size,output_dim=100, weights = [embedding_vector], input_length= 1000, trainable= False))
model.add(LSTM(units=128))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])

X_train, X_test, y_train, y_test = train_test_split(data,label)

model.fit(X_train, y_train, validation_split=0.2, epochs=5)

model.save('news_ChatBot.h5')

y_pred = (model.predict(X_test) >=0.5).astype(int)

accuracy_score(y_test, y_pred)


